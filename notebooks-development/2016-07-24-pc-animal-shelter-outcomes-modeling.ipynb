{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, log_loss\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dframe = pd.read_csv('../data/transformed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'AgeuponOutcome', u'doy', u'Name_hasName', u'Name_noName',\n",
       "       u'AnimalType_Cat', u'AnimalType_Dog', u'SexuponOutcome_Intact Female',\n",
       "       u'SexuponOutcome_Intact Male', u'SexuponOutcome_Neutered Male',\n",
       "       u'SexuponOutcome_Spayed Female', u'Breed_Mix', u'Breed_Pure',\n",
       "       u'dow_Fri', u'dow_Mon', u'dow_Sat', u'dow_Sun', u'dow_Thurs',\n",
       "       u'dow_Tues', u'dow_Wed', u'month_April', u'month_Aug', u'month_Dec',\n",
       "       u'month_Feb', u'month_Jan', u'month_July', u'month_June',\n",
       "       u'month_March', u'month_May', u'month_Nov', u'month_Oct',\n",
       "       u'month_Sept'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dframe.drop('OutcomeType', axis=1)\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = dframe['OutcomeType']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(class_weight=\"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## not sure why this isn't working\n",
    "\n",
    "# kfold = cross_validation.KFold(len(X), n_folds=5)\n",
    "\n",
    "# [rfc.fit(X[train], y[train]).score(X[test], y[test]) for train, test in kfold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pc3sq/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/pc3sq/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/pc3sq/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/pc3sq/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/pc3sq/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.5902439 ,  0.59211553,  0.59312768,  0.59031439,  0.590625  ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross_val_score() does the kfold cross val in previous cell\n",
    "# scores are as good as the best scores on Kaggle; random forest classifier is a good choice\n",
    "cross_validation.cross_val_score(rfc, X, y, cv=5, n_jobs=-1, scoring=\"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.25, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=None, max_features='auto',\n",
       "            max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probas = rfc.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that probas has 5 columns. This is due to the fact that we have 5 outcomes. For each observation, we pick the outcome that has the highest probability out of the 5 probabilities we calculate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6403, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.4182396616559689"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_test, probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Return_to_owner', 'Transfer', 'Adoption', 'Euthanasia', 'Died'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-cf76be918791>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pr' is not defined"
     ]
    }
   ],
   "source": [
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# easy_cf(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import \n",
    "def easy_cf(actuals, predicted):\n",
    "    cf_matrix = confusion_matrix(actuals, predicted)\n",
    "\n",
    "    '''\n",
    "    true positive (TP); eqv. with hit\n",
    "    false positive (FP); eqv. with false alarm, Type I error\n",
    "    true negative (TN); eqv. with correct rejection\n",
    "    false negative (FN); eqv. with miss, Type II error\n",
    "    '''\n",
    "\n",
    "    tn = cf_matrix[0, 0]\n",
    "    fn = cf_matrix[1, 0]\n",
    "    tp = cf_matrix[1, 1]\n",
    "    fp = cf_matrix[0, 1]\n",
    "    total = len(actuals)\n",
    "\n",
    "    pred_pos = tp + fp\n",
    "    pred_neg = tn + fn\n",
    "\n",
    "    act_pos = tp + fn\n",
    "    act_neg = tn + fp\n",
    "\n",
    "    accuracy = (tp + tn) / float(total)\n",
    "\n",
    "    if pred_pos == 0:\n",
    "        recall = \"No predicted +\"\n",
    "        precision = \"No predicted +\"\n",
    "    else:\n",
    "        '''\n",
    "        Recall, or sensitivity (true positive rate), quantifies the avoiding of false negatives.\n",
    "        '''\n",
    "        recall = float(tp)/act_pos\n",
    "\n",
    "        '''\n",
    "        Precision, or positive predictive value (PPV).\n",
    "        If my test predicts positive, how certain can I be that it is a true positive?\n",
    "        '''\n",
    "        precision = float(tp)/pred_pos\n",
    "\n",
    "\n",
    "    if pred_neg == 0:\n",
    "        specificity = \"No predicted -\"\n",
    "    else:\n",
    "        '''\n",
    "        Specificity, or true negative rate, quantifies the avoiding of false positives.\n",
    "        '''\n",
    "        specificity = float(tn)/act_neg\n",
    "\n",
    "\n",
    "    print \"True Positive is:\", tp\n",
    "    print \"False Positive is:\", fp\n",
    "    print \"True Negative is:\", tn\n",
    "    print \"False Negative is:\", fn\n",
    "    print\n",
    "\n",
    "    print \"Predicting + and - correctly\"\n",
    "    print \"Acccuracy:\", accuracy\n",
    "    print\n",
    "\n",
    "    print \"How much can I count on my + prediction?\"\n",
    "    print \"Precision:\", precision\n",
    "    print\n",
    "\n",
    "    print \"Capturing actual + and avoiding FN\"\n",
    "    print \"Sensitivity / Hit Rate / Recall:\", recall\n",
    "    print\n",
    "\n",
    "    print \"Capturing actual - and avoiding FP\"\n",
    "    print \"Specificity / True Negative Rate:\", specificity\n",
    "    print\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feat_importance(rfc_model, X_test):\n",
    "    importances = rfc_model.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in rfc_model.estimators_],\n",
    "                 axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    columns = np.array([   'length', 'percent_nouns',\n",
    "        'subjectivity',      'polarity',    'has_reason', 'has_therefore',\n",
    "          'has_reject',   'has_applied',  'has_standard',      'has_fact',\n",
    "           'has_argue',           'act',        'action',       'appeals',\n",
    "                'case',         'cases',    'certiorari',       'circuit',\n",
    "               'claim',        'claims',         'court',        'courts',\n",
    "                 'did',      'district',          'does',      'evidence',\n",
    "             'federal',         'filed',       'general',          'held',\n",
    "         'information',        'joined',      'judgment',          'jury',\n",
    "                 'law',         'legal',          'make',      'official',\n",
    "             'opinion',    'petitioner',   'petitioners',            'pp',\n",
    "         'respondents',          'rule',          'site',         'state',\n",
    "              'states',       'supreme',           'tax',         'trial',\n",
    "              'united'])\n",
    "\n",
    "    col_importance = columns[indices]\n",
    "\n",
    "\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "\n",
    "    for f in range(X_test.shape[1]):\n",
    "        print(\"%d. feature %d: %s (%f) \" % (f + 1, indices[f], col_importance[f], importances[indices[f]]))\n",
    "\n",
    "    # Plot the feature importances of the forest\n",
    "    plt.figure(num=None, figsize=(20, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.bar(range(X_test.shape[1]), importances[indices],\n",
    "           color=\"r\", yerr=std[indices], align=\"center\")\n",
    "    plt.xticks(range(X_test.shape[1]), indices)\n",
    "    plt.xlim([-1, X_test.shape[1]])\n",
    "    plt.show()\n",
    "\n",
    "    #Plot feature importances of the forest in seaborn\n",
    "    sns.set_style(\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trying gradient boost model\n",
    "gbm = GradientBoostingClassifier(n_estimators=100,random_state=7).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbm_probas = gbm.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_loss(y_test, gbm_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes sense that a gradient boosting classifier works very well because with each new iteration of the estimator, it is placing more weight on the misclassified classes from the previous estimator. Because more weight is placed on previous misclassifications, GBC inherently deals with class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch w/ Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gradient boosting hyperparameters available for tweaking\n",
    "gbm.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dframe[(dframe['OutcomeType']=='Died') | (dframe['OutcomeType']=='Euthanasia')].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# number of features\n",
    "len(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gsearch_gbc = GradientBoostingClassifier(learning_rate=0.1, \n",
    "                                         max_depth=8, \n",
    "                                         max_features=\"auto\", \n",
    "                                         min_samples_leaf=250) # 1623 died or euthansized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first set of params\n",
    "param_test1 = {\"n_estimators\": [50,80],\n",
    "               \"min_samples_split\": [500, 1000]}\n",
    "\n",
    "# run grid search\n",
    "gsearch1 = GridSearchCV(gsearch_gbc, param_grid=param_test1, n_jobs=-1)\n",
    "start1 = time.time()\n",
    "gsearch1.fit(X_train, y_train)\n",
    "end1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gsearch1_probas = gsearch1.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_loss(y_test, gsearch1_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "end1 - start1 # takes just over a minute to run params_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gsearch1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# second set of params\n",
    "param_test2 = {\"n_estimators\": [120, 200],\n",
    "               \"min_samples_split\": [500, 1000]}\n",
    "\n",
    "# run grid search\n",
    "gsearch2 = GridSearchCV(gsearch_gbc, param_grid=param_test2, n_jobs=-1)\n",
    "start2 = time.time()\n",
    "gsearch2.fit(X_train, y_train)\n",
    "end2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "end2 - start2 # takes just over 2 minute to run params_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gsearch2_probas = gsearch2.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_loss(y_test, gsearch2_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gsearch2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# third set of params\n",
    "gsearch_gbc3 = GradientBoostingClassifier(max_depth=8, \n",
    "                                         min_samples_leaf=1000) # 1623 died or euthansized\n",
    "\n",
    "param_test3 = { \"learning_rate\": [0.1, 0.05, 0.01],\n",
    "                \"max_features\": [\"sqrt\", 3, 10]}\n",
    "\n",
    "gsearch3 = GridSearchCV(gsearch_gbc3, param_grid=param_test3, n_jobs=-1)\n",
    "start3 = time.time()\n",
    "gsearch3.fit(X_train, y_train)\n",
    "end3 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "end3 - start3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gsearch3.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gsearch3_probas = gsearch3.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_loss(y_test, gsearch3_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
