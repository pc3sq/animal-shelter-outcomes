{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, log_loss\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dframe = pd.read_csv('../data/transformed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = dframe.drop('OutcomeType', axis=1)\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = dframe['OutcomeType']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(class_weight=\"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## not sure why this isn't working\n",
    "\n",
    "# kfold = cross_validation.KFold(len(X), n_folds=5)\n",
    "\n",
    "# [rfc.fit(X[train], y[train]).score(X[test], y[test]) for train, test in kfold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cross_val_score() does the kfold cross val in previous cell\n",
    "# scores are as good as the best scores on Kaggle; random forest classifier is a good choice\n",
    "cross_validation.cross_val_score(rfc, X, y, cv=5, n_jobs=-1, scoring=\"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.25, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probas = rfc.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that probas has 5 columns. This is due to the fact that we have 5 outcomes. For each observation, we pick the outcome that has the highest probability out of the 5 probabilities we calculate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_loss(y_test, probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# easy_cf(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import \n",
    "def easy_cf(actuals, predicted):\n",
    "    cf_matrix = confusion_matrix(actuals, predicted)\n",
    "\n",
    "    '''\n",
    "    true positive (TP); eqv. with hit\n",
    "    false positive (FP); eqv. with false alarm, Type I error\n",
    "    true negative (TN); eqv. with correct rejection\n",
    "    false negative (FN); eqv. with miss, Type II error\n",
    "    '''\n",
    "\n",
    "    tn = cf_matrix[0, 0]\n",
    "    fn = cf_matrix[1, 0]\n",
    "    tp = cf_matrix[1, 1]\n",
    "    fp = cf_matrix[0, 1]\n",
    "    total = len(actuals)\n",
    "\n",
    "    pred_pos = tp + fp\n",
    "    pred_neg = tn + fn\n",
    "\n",
    "    act_pos = tp + fn\n",
    "    act_neg = tn + fp\n",
    "\n",
    "    accuracy = (tp + tn) / float(total)\n",
    "\n",
    "    if pred_pos == 0:\n",
    "        recall = \"No predicted +\"\n",
    "        precision = \"No predicted +\"\n",
    "    else:\n",
    "        '''\n",
    "        Recall, or sensitivity (true positive rate), quantifies the avoiding of false negatives.\n",
    "        '''\n",
    "        recall = float(tp)/act_pos\n",
    "\n",
    "        '''\n",
    "        Precision, or positive predictive value (PPV).\n",
    "        If my test predicts positive, how certain can I be that it is a true positive?\n",
    "        '''\n",
    "        precision = float(tp)/pred_pos\n",
    "\n",
    "\n",
    "    if pred_neg == 0:\n",
    "        specificity = \"No predicted -\"\n",
    "    else:\n",
    "        '''\n",
    "        Specificity, or true negative rate, quantifies the avoiding of false positives.\n",
    "        '''\n",
    "        specificity = float(tn)/act_neg\n",
    "\n",
    "\n",
    "    print \"True Positive is:\", tp\n",
    "    print \"False Positive is:\", fp\n",
    "    print \"True Negative is:\", tn\n",
    "    print \"False Negative is:\", fn\n",
    "    print\n",
    "\n",
    "    print \"Predicting + and - correctly\"\n",
    "    print \"Acccuracy:\", accuracy\n",
    "    print\n",
    "\n",
    "    print \"How much can I count on my + prediction?\"\n",
    "    print \"Precision:\", precision\n",
    "    print\n",
    "\n",
    "    print \"Capturing actual + and avoiding FN\"\n",
    "    print \"Sensitivity / Hit Rate / Recall:\", recall\n",
    "    print\n",
    "\n",
    "    print \"Capturing actual - and avoiding FP\"\n",
    "    print \"Specificity / True Negative Rate:\", specificity\n",
    "    print\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feat_importance(rfc_model, X_test):\n",
    "    importances = rfc_model.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in rfc_model.estimators_],\n",
    "                 axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    columns = np.array([   'length', 'percent_nouns',\n",
    "        'subjectivity',      'polarity',    'has_reason', 'has_therefore',\n",
    "          'has_reject',   'has_applied',  'has_standard',      'has_fact',\n",
    "           'has_argue',           'act',        'action',       'appeals',\n",
    "                'case',         'cases',    'certiorari',       'circuit',\n",
    "               'claim',        'claims',         'court',        'courts',\n",
    "                 'did',      'district',          'does',      'evidence',\n",
    "             'federal',         'filed',       'general',          'held',\n",
    "         'information',        'joined',      'judgment',          'jury',\n",
    "                 'law',         'legal',          'make',      'official',\n",
    "             'opinion',    'petitioner',   'petitioners',            'pp',\n",
    "         'respondents',          'rule',          'site',         'state',\n",
    "              'states',       'supreme',           'tax',         'trial',\n",
    "              'united'])\n",
    "\n",
    "    col_importance = columns[indices]\n",
    "\n",
    "\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "\n",
    "    for f in range(X_test.shape[1]):\n",
    "        print(\"%d. feature %d: %s (%f) \" % (f + 1, indices[f], col_importance[f], importances[indices[f]]))\n",
    "\n",
    "    # Plot the feature importances of the forest\n",
    "    plt.figure(num=None, figsize=(20, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.bar(range(X_test.shape[1]), importances[indices],\n",
    "           color=\"r\", yerr=std[indices], align=\"center\")\n",
    "    plt.xticks(range(X_test.shape[1]), indices)\n",
    "    plt.xlim([-1, X_test.shape[1]])\n",
    "    plt.show()\n",
    "\n",
    "    #Plot feature importances of the forest in seaborn\n",
    "    sns.set_style(\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trying gradient boost model\n",
    "gbm = GradientBoostingClassifier(n_estimators=100,random_state=7).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbm_probas = gbm.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_loss(y_test, gbm_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes sense that a gradient boosting classifier works very well because with each new iteration of the estimator, it is placing more weight on the misclassified classes from the previous estimator. Because more weight is placed on previous misclassifications, GBC inherently deals with class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch w/ Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gradient boosting hyperparameters available for tweaking\n",
    "gbm.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dframe[(dframe['OutcomeType']=='Died') | (dframe['OutcomeType']=='Euthanasia')].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# number of features\n",
    "len(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gsearch_gbc = GradientBoostingClassifier(learning_rate=0.1, \n",
    "                                         max_depth=8, \n",
    "                                         max_features=\"auto\", \n",
    "                                         min_samples_leaf=250) # 1623 died or euthansized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first set of params\n",
    "param_test1 = {\"n_estimators\": [50,80],\n",
    "               \"min_samples_split\": [500, 1000]}\n",
    "\n",
    "# run grid search\n",
    "gsearch1 = GridSearchCV(gsearch_gbc, param_grid=param_test1, n_jobs=-1)\n",
    "start1 = time.time()\n",
    "gsearch1.fit(X_train, y_train)\n",
    "end1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gsearch1_probas = gsearch1.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_loss(y_test, gsearch1_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "end1 - start1 # takes just over a minute to run params_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gsearch1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# second set of params\n",
    "param_test2 = {\"n_estimators\": [120, 200],\n",
    "               \"min_samples_split\": [500, 1000]}\n",
    "\n",
    "# run grid search\n",
    "gsearch2 = GridSearchCV(gsearch_gbc, param_grid=param_test2, n_jobs=-1)\n",
    "start2 = time.time()\n",
    "gsearch2.fit(X_train, y_train)\n",
    "end2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "end2 - start2 # takes just over 2 minute to run params_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gsearch2_probas = gsearch2.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_loss(y_test, gsearch2_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gsearch2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# third set of params\n",
    "gsearch_gbc3 = GradientBoostingClassifier(max_depth=8, \n",
    "                                         min_samples_leaf=1000) # 1623 died or euthansized\n",
    "\n",
    "param_test3 = { \"learning_rate\": [0.1, 0.05, 0.01],\n",
    "                \"max_features\": [\"sqrt\", 3, 10]}\n",
    "\n",
    "gsearch3 = GridSearchCV(gsearch_gbc3, param_grid=param_test3, n_jobs=-1)\n",
    "start3 = time.time()\n",
    "gsearch3.fit(X_train, y_train)\n",
    "end3 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "end3 - start3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gsearch3.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gsearch3_probas = gsearch3.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_loss(y_test, gsearch3_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get predictions\n",
    "test = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create submission dframe\n",
    "submission = pd.DataFrame(preds, columns=['OutcomeType'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dummify predictions as integers\n",
    "dummy_sub = pd.get_dummies(submission).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# shorten column names\n",
    "dummy_sub.columns = [col.split(\"_\")[1] for col in dummy_sub.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
